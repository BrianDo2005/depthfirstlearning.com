<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-05-10T19:35:47-04:00</updated><id>http://localhost:4000/</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/update/2018/05/10/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2018-05-10T18:50:23-04:00</published><updated>2018-05-10T18:50:23-04:00</updated><id>http://localhost:4000/jekyll/update/2018/05/10/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/05/10/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">Trust Region Policy Optimization</title><link href="http://localhost:4000/reinforcement-learning/2018/05/10/trpo.html" rel="alternate" type="text/html" title="Trust Region Policy Optimization" /><published>2018-05-10T18:50:23-04:00</published><updated>2018-05-10T18:50:23-04:00</updated><id>http://localhost:4000/reinforcement-learning/2018/05/10/trpo</id><content type="html" xml:base="http://localhost:4000/reinforcement-learning/2018/05/10/trpo.html">&lt;h1 id=&quot;trust-region-policy-optimization&quot;&gt;Trust Region Policy Optimization&lt;/h1&gt;

&lt;h2 id=&quot;policy-gradients&quot;&gt;Policy Gradients&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics&quot;&gt;General high level topics:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce MDP setting&lt;/li&gt;
  &lt;li&gt;REINFORCE theorem&lt;/li&gt;
  &lt;li&gt;Continuous action spaces&lt;/li&gt;
  &lt;li&gt;On-policy setting&lt;/li&gt;
  &lt;li&gt;Policy gradient&lt;/li&gt;
  &lt;li&gt;Why does policy gradient have such high variance?&lt;/li&gt;
  &lt;li&gt;Why does step size matter?&lt;/li&gt;
  &lt;li&gt;Off-policy vs on-policy?&lt;/li&gt;
  &lt;li&gt;Why PG for continuous action spaces?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CS 294 Lecture 4 about Policy Gradient (main source):
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tWNpiNzWuO8&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;index=4&quot;&gt;Video&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;David Silver’s class slides about PG (main source):
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KHZVXao4qXs&amp;amp;index=7&amp;amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT&quot;&gt;Video&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(Optional, but highly recommended) &lt;a href=&quot;http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/2016-MLSS-RL.pdf&quot;&gt;John Schulman introduction at MLSS Cadiz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(Optional) &lt;a href=&quot;http://rll.berkeley.edu/deeprlcoursesp17/docs/lec6.pdf&quot;&gt;Lecture on Variance Reduction for Policy Gradient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;(Optional) &lt;a href=&quot;http://karpathy.github.io/2016/05/31/rl/&quot;&gt;Introduction to policy gradient and motivations by Andrej Karpathy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;papers&quot;&gt;Papers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sutton and Barto 2nd Edition, pages 265 - 273: [Textbook]&lt;/li&gt;
  &lt;li&gt;(&lt;strong&gt;Highly recommended&lt;/strong&gt;) &lt;a href=&quot;http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf&quot;&gt;Simple statistical gradient-following algorithms for connectionist reinforcement learning&lt;/a&gt; (REINFORCE, Williams, 1992)&lt;/li&gt;
  &lt;li&gt;(Optional) &lt;a href=&quot;http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf&quot;&gt;Connection Between Importance Sampling and Likelihood Ratio&lt;/a&gt; (Tang and Abbeel, 2010)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;variance-reduction-and-advantage-estimate&quot;&gt;Variance Reduction and Advantage Estimate&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics-of-discussion&quot;&gt;General high level topics of discussion:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;REINFORCE / Vanilla PG&lt;/li&gt;
  &lt;li&gt;Likelihood Ratio&lt;/li&gt;
  &lt;li&gt;Importance Sampling connection&lt;/li&gt;
  &lt;li&gt;Show why no dynamics model is needed&lt;/li&gt;
  &lt;li&gt;Variance reduction: Baselines&lt;/li&gt;
  &lt;li&gt;Variance reduction: Causality&lt;/li&gt;
  &lt;li&gt;Advantage Estimation / GAE?&lt;/li&gt;
  &lt;li&gt;Final Algorithm&lt;/li&gt;
  &lt;li&gt;Why does it fail?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resources-1&quot;&gt;Resources&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CS 294 Lecture 5 about Actor Critic Algorithms
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PpVhtJn-iZI&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;amp;index=5&quot;&gt;Video&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;George Tucker’s notes on Variance Reduction&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;papers-1&quot;&gt;Papers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sutton and Barto 2nd Edition, pages 273 - 275: [Textbook]&lt;/li&gt;
  &lt;li&gt;(Optional) &lt;a href=&quot;https://arxiv.org/abs/1506.02438&quot;&gt;High-dimensional continuous control using generalized advantage estimation&lt;/a&gt; (GAE)&lt;/li&gt;
  &lt;li&gt;(Optional) &lt;a href=&quot;https://arxiv.org/abs/1602.01783&quot;&gt;Asynchronous Methods for Deep Reinforcement Learning&lt;/a&gt; (A3C)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fisher-information-matrix-and-natural-gradient-descent&quot;&gt;Fisher Information Matrix and Natural Gradient Descent&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics-of-discussion-1&quot;&gt;General high level topics of discussion:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Fisher Information Matrix&lt;/li&gt;
  &lt;li&gt;Natural Gradient Descent&lt;/li&gt;
  &lt;li&gt;(Optional) K-Fac&lt;/li&gt;
  &lt;li&gt;Gradient descent as Euclidean steepest descent&lt;/li&gt;
  &lt;li&gt;Fisher Matrix
    &lt;ul&gt;
      &lt;li&gt;How is it similar, and different from the Hessian&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Two equivalent definitions&lt;/li&gt;
  &lt;li&gt;Natural gradient descent, compared to Newton’s method&lt;/li&gt;
  &lt;li&gt;Why is the natural gradient slow to compute?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;main-resources-required-reading&quot;&gt;Main Resources (required reading)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.1193.pdf&quot;&gt;New insights and perspectives on the natural gradient method&lt;/a&gt; / James Martens (Sections 1-11; ends at page 24)&lt;/li&gt;
  &lt;li&gt;Matt Johnson’s Natural Gradient Descent and K-Fac Tutorial (Sections 1-7, Section A, Section B)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.archive.org/web/20170807004738/https://hips.seas.harvard.edu/blog/2013/04/08/fisher-information/&quot;&gt;Fisher Information Matrix&lt;/a&gt; from Ryan Adams’ lab&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optional-resources&quot;&gt;Optional resources&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;The rest of Matt Johnson’s tutorial above&lt;/li&gt;
  &lt;li&gt;The rest of James Martens’ survey paper above&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;&gt;Wikipedia on Fisher Information&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ipvs.informatik.uni-stuttgart.de/mlr/wp-content/uploads/2015/01/mathematics_for_intelligent_systems_lecture12_notes_I.pdf&quot;&gt;8-page intro to natural gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;papers-optional&quot;&gt;Papers (optional)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yaroslavvb.com/papers/amari-why.pdf&quot;&gt;Why Natural Gradient Descent / Amari and Douglas&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://personalrobotics.ri.cmu.edu/files/courses/papers/Amari1998a.pdf&quot;&gt;Natural Gradient Works Efficiently in Learning / Amari&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;books-very-optional&quot;&gt;Books (very optional)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;A book by Amari on Information Geometry: &lt;a href=&quot;http://www.springer.com/gp/book/9784431559771&quot;&gt;Springer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conjugate-gradients&quot;&gt;Conjugate Gradients&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics-of-discussion-2&quot;&gt;General high level topics of discussion:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Solving system of linear equations&lt;/li&gt;
  &lt;li&gt;Efficiently computing matrix-vector products&lt;/li&gt;
  &lt;li&gt;Natural Gradient descent requires computing F^{-1} \grad{f}&lt;/li&gt;
  &lt;li&gt;Generally, 2nd order methods require computing B^{-1} \grad{f} for some B&lt;/li&gt;
  &lt;li&gt;How to solve efficiently for the gradient direction?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;main-resources-required-reading-1&quot;&gt;Main Resources (required reading)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf&quot;&gt;An Introduction to the Conjugate Gradient Method Without the Agonizing Pain&lt;/a&gt; (Section 7 to 9)&lt;/li&gt;
  &lt;li&gt;Convex Optimization II by Stephen Boyd
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?feature=player_embedded&amp;amp;v=cHVpwyYU_LY#t=2230&quot;&gt;Lecture 12, from 37:10 to 1:05:00&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?feature=player_embedded&amp;amp;v=E4gl91l0l40#t=1266&quot;&gt;Lecture 13, from 21:20 to 29:30&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optional-references-book-&quot;&gt;Optional references (Book) :&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Numerical Optimization by Nocedal, Wright&lt;/li&gt;
  &lt;li&gt;Section 5.1, “The linear conjugate gradient method,” up through “A practical form of the conjugate gradient method,”&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references-&quot;&gt;References :&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://metacademy.org/graphs/concepts/conjugate_gradient&quot;&gt;Metacademy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trust-region-methods&quot;&gt;Trust Region Methods&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics-of-discussion-3&quot;&gt;General high level topics of discussion:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;What are trust regions?&lt;/li&gt;
  &lt;li&gt;How do we solve problems with trust regions?&lt;/li&gt;
  &lt;li&gt;What are trust region methods and line search methods?&lt;/li&gt;
  &lt;li&gt;What is a Cauchy point?&lt;/li&gt;
  &lt;li&gt;How can we efficiently solve the trust region subproblems?
    &lt;ul&gt;
      &lt;li&gt;Dog-leg&lt;/li&gt;
      &lt;li&gt;2-dimensional search method&lt;/li&gt;
      &lt;li&gt;Truncated conjugate gradient&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What are the advantages and drawbacks of trust region methods?&lt;/li&gt;
  &lt;li&gt;Why are they especially good for RL?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;main-resources-required-reading-2&quot;&gt;Main Resources (required reading)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Nocedal and Wright, Numerical Optimization:
    &lt;ul&gt;
      &lt;li&gt;Chapter 2 (overview – feel free to skim)&lt;/li&gt;
      &lt;li&gt;Chapter 4, Section 4.1, 4.2&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A review of trust region algorithms for optimization&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optional-references&quot;&gt;Optional references&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods&quot;&gt;A light, more friendly introduction to trust region methods&lt;/a&gt; (feel free to start with this)&lt;/li&gt;
  &lt;li&gt;Nocedal and Wright, Numerical Optimization:
    &lt;ul&gt;
      &lt;li&gt;Chapter 4, Section 4.3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;TRPO&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.05144&quot;&gt;ACKTR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trpo&quot;&gt;TRPO&lt;/h2&gt;

&lt;h3 id=&quot;general-high-level-topics-of-discussion-4&quot;&gt;General high level topics of discussion:&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;What is the problem we are trying to solve?&lt;/li&gt;
  &lt;li&gt;What are the bottlenecks in existing approaches?&lt;/li&gt;
  &lt;li&gt;Why TRPO?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;What is the connection between TRPO, PPO, and CPO?&lt;/li&gt;
  &lt;li&gt;In practice, TRPO is really slow. What is the main computational bottleneck, and how might we remove it?&lt;/li&gt;
  &lt;li&gt;What is Policy Improvement? (and monotic improvement theory)&lt;/li&gt;
  &lt;li&gt;Why use conjugate gradient methods for optimization? (Can we exploit the fact the conjugate gradient optimization is differentiable?)&lt;/li&gt;
  &lt;li&gt;How is line search used in TRPO?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;main-resources-required-reading-3&quot;&gt;Main Resources (required reading)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05477&quot;&gt;Trust Region Policy Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;DRL Course @Berkeley
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ycCtmp4hcUs&amp;amp;feature=youtu.be&amp;amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&quot;&gt;Video&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optional-references-1&quot;&gt;Optional references&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf&quot;&gt;Approximately Optimal Approximate Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://reinforce.io/blog/end-to-end-computation-graphs-for-reinforcement-learning/&quot;&gt;TRPO Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Trust Region Policy Optimization</summary></entry></feed>