<!DOCTYPE html>
<html>
    <head>
        <title>Depth First Learning</title>
        <link rel="stylesheet" type="text/css" href="/css/main.css">
    </head>
    <body>
        <nav>
            <ul>
                <li><a href="/">Home</a></li>
                <li><a href="/about">About</a></li>
                <li><a href="/curriculums">Curriculums</a></li>
            </ul>
        </nav>
        <div class="container">

            <h1>Trust Region Policy Optimization</h1>
<p class="meta">10 May 2018</p>

<div class="post">
    <h1 id="trust-region-policy-optimization">Trust Region Policy Optimization</h1>

<h2 id="policy-gradients">Policy Gradients</h2>

<h3 id="general-high-level-topics">General high level topics:</h3>
<ul>
  <li>Introduce MDP setting</li>
  <li>REINFORCE theorem</li>
  <li>Continuous action spaces</li>
  <li>On-policy setting</li>
  <li>Policy gradient</li>
  <li>Why does policy gradient have such high variance?</li>
  <li>Why does step size matter?</li>
  <li>Off-policy vs on-policy?</li>
  <li>Why PG for continuous action spaces?</li>
</ul>

<h3 id="resources">Resources</h3>
<ul>
  <li>CS 294 Lecture 4 about Policy Gradient (main source):
    <ul>
      <li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_4_policy_gradient.pdf">Slides</a></li>
      <li><a href="https://www.youtube.com/watch?v=tWNpiNzWuO8&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;index=4">Video</a></li>
    </ul>
  </li>
  <li>David Silver’s class slides about PG (main source):
    <ul>
      <li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf">Slides</a></li>
      <li><a href="https://www.youtube.com/watch?v=KHZVXao4qXs&amp;index=7&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT">Video</a></li>
    </ul>
  </li>
  <li>(Optional, but highly recommended) <a href="http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/2016-MLSS-RL.pdf">John Schulman introduction at MLSS Cadiz</a></li>
  <li>(Optional) <a href="http://rll.berkeley.edu/deeprlcoursesp17/docs/lec6.pdf">Lecture on Variance Reduction for Policy Gradient</a></li>
  <li>(Optional) <a href="http://karpathy.github.io/2016/05/31/rl/">Introduction to policy gradient and motivations by Andrej Karpathy</a></li>
</ul>

<h3 id="papers">Papers</h3>
<ul>
  <li>Sutton and Barto 2nd Edition, pages 265 - 273: [Textbook]</li>
  <li>(<strong>Highly recommended</strong>) <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf">Simple statistical gradient-following algorithms for connectionist reinforcement learning</a> (REINFORCE, Williams, 1992)</li>
  <li>(Optional) <a href="http://rll.berkeley.edu/~jietang/pubs/nips10_Tang.pdf">Connection Between Importance Sampling and Likelihood Ratio</a> (Tang and Abbeel, 2010)</li>
</ul>

<h2 id="variance-reduction-and-advantage-estimate">Variance Reduction and Advantage Estimate</h2>

<h3 id="general-high-level-topics-of-discussion">General high level topics of discussion:</h3>
<ul>
  <li>REINFORCE / Vanilla PG</li>
  <li>Likelihood Ratio</li>
  <li>Importance Sampling connection</li>
  <li>Show why no dynamics model is needed</li>
  <li>Variance reduction: Baselines</li>
  <li>Variance reduction: Causality</li>
  <li>Advantage Estimation / GAE?</li>
  <li>Final Algorithm</li>
  <li>Why does it fail?</li>
</ul>

<h3 id="resources-1">Resources</h3>
<ul>
  <li>CS 294 Lecture 5 about Actor Critic Algorithms
    <ul>
      <li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf">Slides</a></li>
      <li><a href="https://www.youtube.com/watch?v=PpVhtJn-iZI&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&amp;index=5">Video</a></li>
    </ul>
  </li>
  <li>George Tucker’s notes on Variance Reduction</li>
</ul>

<h3 id="papers-1">Papers</h3>
<ul>
  <li>Sutton and Barto 2nd Edition, pages 273 - 275: [Textbook]</li>
  <li>(Optional) <a href="https://arxiv.org/abs/1506.02438">High-dimensional continuous control using generalized advantage estimation</a> (GAE)</li>
  <li>(Optional) <a href="https://arxiv.org/abs/1602.01783">Asynchronous Methods for Deep Reinforcement Learning</a> (A3C)</li>
</ul>

<h2 id="fisher-information-matrix-and-natural-gradient-descent">Fisher Information Matrix and Natural Gradient Descent</h2>

<h3 id="general-high-level-topics-of-discussion-1">General high level topics of discussion:</h3>
<ul>
  <li>Fisher Information Matrix</li>
  <li>Natural Gradient Descent</li>
  <li>(Optional) K-Fac</li>
  <li>Gradient descent as Euclidean steepest descent</li>
  <li>Fisher Matrix
    <ul>
      <li>How is it similar, and different from the Hessian</li>
    </ul>
  </li>
  <li>Two equivalent definitions</li>
  <li>Natural gradient descent, compared to Newton’s method</li>
  <li>Why is the natural gradient slow to compute?</li>
</ul>

<h3 id="main-resources-required-reading">Main Resources (required reading)</h3>
<ul>
  <li><a href="https://arxiv.org/pdf/1412.1193.pdf">New insights and perspectives on the natural gradient method</a> / James Martens (Sections 1-11; ends at page 24)</li>
  <li>Matt Johnson’s Natural Gradient Descent and K-Fac Tutorial (Sections 1-7, Section A, Section B)</li>
  <li><a href="https://web.archive.org/web/20170807004738/https://hips.seas.harvard.edu/blog/2013/04/08/fisher-information/">Fisher Information Matrix</a> from Ryan Adams’ lab</li>
</ul>

<h4 id="optional-resources">Optional resources</h4>
<ul>
  <li>The rest of Matt Johnson’s tutorial above</li>
  <li>The rest of James Martens’ survey paper above</li>
  <li><a href="https://en.wikipedia.org/wiki/Fisher_information">Wikipedia on Fisher Information</a></li>
  <li><a href="http://ipvs.informatik.uni-stuttgart.de/mlr/wp-content/uploads/2015/01/mathematics_for_intelligent_systems_lecture12_notes_I.pdf">8-page intro to natural gradients</a></li>
</ul>

<h4 id="papers-optional">Papers (optional)</h4>
<ul>
  <li><a href="http://www.yaroslavvb.com/papers/amari-why.pdf">Why Natural Gradient Descent / Amari and Douglas</a></li>
  <li><a href="https://personalrobotics.ri.cmu.edu/files/courses/papers/Amari1998a.pdf">Natural Gradient Works Efficiently in Learning / Amari</a></li>
</ul>

<h4 id="books-very-optional">Books (very optional)</h4>
<ul>
  <li>A book by Amari on Information Geometry: <a href="http://www.springer.com/gp/book/9784431559771">Springer</a></li>
</ul>

<h2 id="conjugate-gradients">Conjugate Gradients</h2>

<h3 id="general-high-level-topics-of-discussion-2">General high level topics of discussion:</h3>
<ul>
  <li>Solving system of linear equations</li>
  <li>Efficiently computing matrix-vector products</li>
  <li>Natural Gradient descent requires computing F^{-1} \grad{f}</li>
  <li>Generally, 2nd order methods require computing B^{-1} \grad{f} for some B</li>
  <li>How to solve efficiently for the gradient direction?</li>
</ul>

<h3 id="main-resources-required-reading-1">Main Resources (required reading)</h3>
<ul>
  <li><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">An Introduction to the Conjugate Gradient Method Without the Agonizing Pain</a> (Section 7 to 9)</li>
  <li>Convex Optimization II by Stephen Boyd
    <ul>
      <li><a href="https://www.youtube.com/watch?feature=player_embedded&amp;v=cHVpwyYU_LY#t=2230">Lecture 12, from 37:10 to 1:05:00</a></li>
      <li><a href="https://www.youtube.com/watch?feature=player_embedded&amp;v=E4gl91l0l40#t=1266">Lecture 13, from 21:20 to 29:30</a></li>
    </ul>
  </li>
</ul>

<h4 id="optional-references-book-">Optional references (Book) :</h4>
<ul>
  <li>Numerical Optimization by Nocedal, Wright</li>
  <li>Section 5.1, “The linear conjugate gradient method,” up through “A practical form of the conjugate gradient method,”</li>
</ul>

<h3 id="references-">References :</h3>
<ul>
  <li><a href="https://metacademy.org/graphs/concepts/conjugate_gradient">Metacademy</a></li>
</ul>

<h2 id="trust-region-methods">Trust Region Methods</h2>

<h3 id="general-high-level-topics-of-discussion-3">General high level topics of discussion:</h3>
<ul>
  <li>What are trust regions?</li>
  <li>How do we solve problems with trust regions?</li>
  <li>What are trust region methods and line search methods?</li>
  <li>What is a Cauchy point?</li>
  <li>How can we efficiently solve the trust region subproblems?
    <ul>
      <li>Dog-leg</li>
      <li>2-dimensional search method</li>
      <li>Truncated conjugate gradient</li>
    </ul>
  </li>
  <li>What are the advantages and drawbacks of trust region methods?</li>
  <li>Why are they especially good for RL?</li>
</ul>

<h3 id="main-resources-required-reading-2">Main Resources (required reading)</h3>
<ul>
  <li>Nocedal and Wright, Numerical Optimization:
    <ul>
      <li>Chapter 2 (overview – feel free to skim)</li>
      <li>Chapter 4, Section 4.1, 4.2</li>
    </ul>
  </li>
  <li>A review of trust region algorithms for optimization</li>
</ul>

<h4 id="optional-references">Optional references</h4>
<ul>
  <li><a href="https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods">A light, more friendly introduction to trust region methods</a> (feel free to start with this)</li>
  <li>Nocedal and Wright, Numerical Optimization:
    <ul>
      <li>Chapter 4, Section 4.3</li>
    </ul>
  </li>
  <li><a href="https://arxiv.org/abs/1502.05477">TRPO</a></li>
  <li><a href="https://arxiv.org/abs/1708.05144">ACKTR</a></li>
</ul>

<h2 id="trpo">TRPO</h2>

<h3 id="general-high-level-topics-of-discussion-4">General high level topics of discussion:</h3>
<ul>
  <li>What is the problem we are trying to solve?</li>
  <li>What are the bottlenecks in existing approaches?</li>
  <li>Why TRPO?</li>
</ul>

<h3 id="overview">Overview</h3>
<ul>
  <li>What is the connection between TRPO, PPO, and CPO?</li>
  <li>In practice, TRPO is really slow. What is the main computational bottleneck, and how might we remove it?</li>
  <li>What is Policy Improvement? (and monotic improvement theory)</li>
  <li>Why use conjugate gradient methods for optimization? (Can we exploit the fact the conjugate gradient optimization is differentiable?)</li>
  <li>How is line search used in TRPO?</li>
</ul>

<h3 id="main-resources-required-reading-3">Main Resources (required reading)</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a></li>
  <li>DRL Course @Berkeley
    <ul>
      <li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_13_advanced_pg.pdf">Slides</a></li>
      <li><a href="https://www.youtube.com/watch?v=ycCtmp4hcUs&amp;feature=youtu.be&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3">Video</a></li>
    </ul>
  </li>
</ul>

<h4 id="optional-references-1">Optional references</h4>
<ul>
  <li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a></li>
  <li><a href="https://reinforce.io/blog/end-to-end-computation-graphs-for-reinforcement-learning/">TRPO Tutorial</a></li>
</ul>

</div>


        </div><!-- /.container -->
        <footer>
            <ul>
                <li><a href="mailto:depthfirstlearning@gmail.com">email</a></li>
            </ul>
        </footer>
    </body>
</html>
