# InfoGAN

The full paper is here: [https://arxiv.org/abs/1606.03657](https://arxiv.org/abs/1606.03657), originally from NIPS 2016.

## Why is this paper important?

InfoGAN is an extension of GAN, which learns to map unlabelled data to codes (aka representation learning). Compare this to vanilla GANs that can only generate samples, or to another generative model, VAE, that learns to both generate codes and samples. Representation learning is one of the biggest promises of unsupservised learning, and GANs are one of the most flexible and powerful such models. This makes InfoGAN important and an interesting stepping stone towards possible future research in representation learning.

## Study Plan

1. [Information Theory](#1-information-theory)
2. [GANs](#2-gans-generative-adversarial-networks)
3. [The InfoGAN paper](#3-the-infogan-paper)

***

## 1. Information Theory

Information theory was originally derived for the analysis of error-correcting codes, but that's not what we care about here. Information theory formalizes the concept of the "amount of randomness" or "amount of information" in random variables. Moreover, these concepts can be extended to relative quantities between different random variables.

Intuitively, a coin that always falls on heads has no randomness, meaning that each observation of its result contains no information. This means that the entropy of the random variable representing the outcome of the coin flip is 0. If the coin is fair, then each observed flip contains exactly one "bit" of information, hence the entropy of such a coin flip is said to be "1 bit".

This section leads up to Mutual Information, which extends entropy to how much additional information you get from observing a joint sample from two random variables, as compared to the baseline of observing each random variable separately. Mutual information is the core concept that's used in the formulation of InfoGAN, the paper that this series leads up to.

### General high-level topics:

 - Entropy
 - Differential Entropy
 - Conditional Entropy
 - Jensen’s Inequality
 - KL divergence
 - Mutual Information

### Resources

 - Chapter 1.6 from Pattern Recognition and Machine Learning / Bishop. (book colloquially known as "PRML")
 - A good [intuitive explanation of Entropy](https://www.quora.com/What-is-an-intuitive-explanation-of-the-concept-of-entropy-in-information-theory/answer/Peter-Gribble), from Quora.
 - For more perspectives and deeper dependencies, see Metacademy:
   - [Entropy](https://metacademy.org/graphs/concepts/entropy)
   - [Mutual Information](https://metacademy.org/graphs/concepts/mutual_information)
   - [KL diverence](https://metacademy.org/graphs/concepts/kl_divergence)
   - (Optional) [Notes on Kullback-Leibler Divergence and Likelihood Theory](https://arxiv.org/pdf/1404.2000.pdf)

### Questions
 - From PRML: 1.28, 1.31, 1.36, 1.37, 1.38, 1.39, 1.41.
 - How is Mutual Information similar to correlation? How are they different? Are they directly related under some conditions?
 - In classification problems, [minimizing cross-entropy loss is the same as minimizing the KL divergence 
   of the predicted class distribution from the true class distribution](https://ai.stackexchange.com/questions/3065/why-has-cross-entropy-become-the-classification-standard-loss-function-and-not-k/4185). Why do we minimize the KL, rather
   than other measures, such as L2 distance?
   <details><summary>One possible answer</summary>
   In classification problem: One natural measure of “goodness” is the likelihood or marginal prob of observed values. By definition, it’s P(Y | X; params), which is Sum_i P(Y = yi | X; params). This says that we want to maximize the probability of producing the “correct” yi class only, and don’t really care to push down the probability of incorrect class like L2 loss would.
   <br />
   E.g., suppose the true label y = [0, 1, 0] (one-hot of class label {1, 2, 3}), and the softmax of the final layer in NN is y’ = [0.2, 0.5, 0.3]. One could use L2 between these two distributions, but if instead we minimize KL divergence KL(y || y’), which is equivalent to minimizing cross-entropy loss (the standard loss everyone uses to solve this problem), we would compute 0 * log(0) + 1 * log (0.5) + 0 * log(0) = log(0.5), which describes exactly the log likelihood of the label being class 2 for this particular training example. Here choosing to minimize KL means we’re maximizing the data likelihood. I think it could also be reasonable to use L2, but we would be maximizing the data likelihood + “unobserved anti-likelihood” :) (my made up word) meaning we want to kill off all those probabilities of predicting wrong labels as well. Another reason L2 is less prefered might be that L2 involves looping over all class labels whereas KL can look only at the correct class when computing the loss.
   </details>

***

## 2. GANs (Generative Adversarial Networks)

GANs are framework for constructing models that learn to sample from a
probability distribution, given a finite sample from that distribution.
More concretely, after training on a finite unlabeled dataset (say of images), 
a GAN can generate new images from the same "kind" that aren't in the original
training set.

GANs are most commonly known for their ability to generate realistic
looking images when trained on datasets like CIFAR-10, CelebA or ImageNet.

### General high level topics:
 - JS (Jensen-Shannon) divergence
 - How are GANs trained?
 - Various possible GAN objectives. Why are they needed?
 - GAN training minimizes the JS divergence between the data-generating distribution and the distribution of samples from the generator part of the GAN

### Resources
 - [JS Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)
 - [The original GAN paper](https://arxiv.org/abs/1406.2661)

### Questions
  - Prove that 0 <= JSD(P||Q) <= 1 bit for all P, Q. When are the bounds achieved?
    <details><summary>Answer</summary>Start <a href="https://en.wikipedia.org/wiki/Jensen-Shannon_divergence#Relation_to_mutual_information">here</a>
    </details>
  - What are the bounds for KL divergence? When are those bounds achieved?
<!--- TODO:  - Why is it called the Jensen-Shannon divergence? --->
  - In the paper, why do they say “In practice, equation 1 may not provide sufficient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, log(1 − D(G(z))) saturates”?
    <details><summary>A sketch of an answer</summary>
      <a href="/assets/gan_gradient.pdf">Understanding the vanishing generator gradients point in the GAN paper</a>
    </details>
  - Implement a [Colab](https://colab.research.google.com/) that trains a GAN for MNIST. Try both the saturating and non-saturating discriminator loss.

***

## 3. The InfoGAN paper

We now get to the [paper itself](https://arxiv.org/abs/1606.03657). InfoGAN modifies the original GAN objective thus:
 - Split the incoming noise vector z into two parts -- z (noise) and c (code). The goal is to learn meaningful codes for the dataset.
 - It achieves this by adding another prediction head to the network (in addition to the discriminator) that tries to predict c from the generated sample. The loss is a combination of the normal GAN loss and the loss on the new predictions of c.
 - This new loss term can be interpreted as a lower bound on the mutual information between the generated samples and the code.

### General high-level topics:

 - The InfoGAN objective
 - Why can't we directly optimize for the mutual information I(c; G(z,c))
 - Variational Information Maximization
 - Possible choices for classes of random variables for dimensions of the code c

### Resources

 - The [InfoGAN paper](https://arxiv.org/abs/1606.03657)
 - [A correction to a proof in the paper](http://aoliver.org/assets/correct-proof-of-infogan-lemma.pdf)
 - [A sample InfoGAN implementation Colab](https://drive.google.com/file/d/1JkCI_n2U2i6DFU8NKk3P6EkPo3ZTKAaq/view?usp=sharing)

### Questions

 - How does one compute log Q(c|x) in practice? How does this answer change based on the choice of the type of random variables in c?
    <details><summary>Hint</summary>
      What is log Q(c|x) when c is a Gaussian centered at f_theta(x)? What about when c is the output of a softmax?
      <br />
      See section 6.
    </details>
 - Which objective in the paper can actually be optimized with gradient-based algorithms? How? (An answer to this needs to refer to "the reparameterization trick")
 - Why is an auxiliary Q distribution necessary?
 - Draw a neural network diagram for InfoGAN
   <details><summary>Answer</summary>
     There is a good diagram in <a href="https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd">this blog post</a>
   </details>
 - In the paper they say "However, in this paper we opt for
simplicity by fixing the latent code distribution and we will treat H(c) as a constant.". What if you want to learn
the latent code (say, if you don't know that classes are balanced in the dataset). Can you still optimize for this with gradient-based algorithms? Can you implement this on an intentionally class-imbalanced variant of MNIST?
    <details><summary>A sketch of an answer</summary>
    You could imagine learning the parameters of the distribution of c, if you can get H(c) to be a differentiable function of those parameters.
    </details>
 - In the paper they say "the lower bound ... is quickly maximized to ... and maximal mutual information is achieved". How do they know this is the maximal value?
 - Open-ended question: Is InfoGAN guaranteed to find disentangled representations? How would you tell if a representation is disentangled?
